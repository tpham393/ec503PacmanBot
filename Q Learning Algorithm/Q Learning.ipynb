{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nQ-learning algorithm\\n\"Stochastic formulation\" of Value Iteration policy. Uses a slightly different update equation, such that information\\nabout the state transition probabilities is no longer needed. Introduces the concept of exploration vs exploitation, i.e.\\nhow to choose the value of epsilon/alpha (the learning rate) of the agent.\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Q-learning algorithm\n",
    "\"Stochastic formulation\" of Value Iteration policy. Uses a slightly different update equation, such that information\n",
    "about the state transition probabilities is no longer needed. Introduces the concept of exploration vs exploitation, i.e.\n",
    "how to choose the value of epsilon/alpha (the learning rate) of the agent.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "UP = 0\n",
    "RIGHT = 1\n",
    "DOWN = 2\n",
    "LEFT = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Enumerate states for a 3x3 grid ==> 81 states \n",
    "(9 choices for pacman location x 9 choices for ghost location)\n",
    "\n",
    "y    _0_|_1_|_2_\n",
    "|    _3_|_4_|_5_\n",
    "v     6 | 7 | 8\n",
    "    x -->\n",
    "\n",
    "Each x,y pair represented as an integer number corresponding to the diagram above\n",
    "'''\n",
    "\n",
    "states_num = [];\n",
    "\n",
    "for s in range(81):\n",
    "    for p in range(9):\n",
    "        for g in range(9):\n",
    "            states_num.append( (p, g) )\n",
    "                    \n",
    "#for s in range(81):\n",
    "#    print(\"state \", s, \": \", states_num[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Define some global methods. Previously part of PacmanEnv class but extracted b/c they'll\n",
    "also be used by the Q-learning algorithm function\n",
    "'''\n",
    "def grid_to_xy(number):\n",
    "    switch = {\n",
    "        0: [0,0],\n",
    "        1: [1,0],\n",
    "        2: [2,0],\n",
    "        3: [0,1],\n",
    "        4: [1,1],\n",
    "        5: [2,1],\n",
    "        6: [0,2],\n",
    "        7: [1,2],\n",
    "        8: [2,2]\n",
    "    }\n",
    "    return switch.get(number, \"invalid entry\")\n",
    "\n",
    "def xy_to_grid(x,y):\n",
    "    switch = {\n",
    "        0: {0:0, 1:3, 2:6},\n",
    "        1: {0:1, 1:4, 2:7},\n",
    "        2: {0:2, 1:5, 2:8}\n",
    "    }\n",
    "    x = switch.get(x,\"invalid entry\")\n",
    "\n",
    "    if x == \"invalid entry\":\n",
    "        return x\n",
    "    else:\n",
    "        return x.get(y,\"invalid entry\")\n",
    "\n",
    "def return_state(states, p, g):\n",
    "    return states.index( (p,g) )\n",
    "\n",
    "def move(x, y, action):\n",
    "    if action == UP:\n",
    "        y = max(0, y-1)\n",
    "    elif action == RIGHT:\n",
    "        x = min(2, x+1)\n",
    "    elif action == DOWN:\n",
    "        y = min(2, y+1)\n",
    "    elif action == LEFT:\n",
    "        x = max(0, x-1)\n",
    "    return xy_to_grid(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PacmanEnv:\n",
    "    '''\n",
    "    Class to initialize and store information about the Pacman environment for program planning algorithms.\n",
    "\n",
    "    Properties:\n",
    "        P[s][a] is a list of is a list of transition tuples (prob, next_state, reward, done)\n",
    "        num_states = number of states (set to default for 3x3 grid)\n",
    "        num_actions = number of actions (set to 4)\n",
    "        pellet_loc = location of pellet (set to 2, i.e. [2,0] by default)\n",
    "\n",
    "    Methods:\n",
    "        return_state: Returns state number given location of pacman and the ghost\n",
    "        move: Moves pacman given current location and action input. Returns grid location number\n",
    "        calculate_reward: Returns reward for current location of pacman. Used to evaluate R(s,a,s') by \n",
    "                        first determining s' through move(s,a), then calculating the reward at s'.\n",
    "        grid_to_xy: Returns corresponding (x,y) coordinate pair for valid grid location integer input\n",
    "                    If number out of range, returns 'invalid entry' error message\n",
    "        xy_to_grid: Returns corresponding grid location # for given (x,y) coordinate pair input\n",
    "                    If number out of range, returns 'invalid entry' error message\n",
    "    '''\n",
    "\n",
    "    def __init__(self, states=states_num, num_states=81, num_actions=4, pellet_loc=2):\n",
    "        self.states = states\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.pellet_loc = pellet_loc\n",
    "        \n",
    "        P = {s : {a : [] for a in range(num_actions)} for s in range(num_states)}\n",
    "        \n",
    "        # parameters must be of the same type, i.e. [x,y] or int value 0-8\n",
    "        # need to adjust to include reward definition for bumping into walls\n",
    "        def calculate_reward(pacman_new_loc, ghost_new_loc, ghost_current_loc, pellet_location):\n",
    "            if pacman_new_loc == ghost_current_loc: # pacman moved to the ghost's location\n",
    "                return -1000\n",
    "            elif pacman_new_loc == pellet_location:\n",
    "                return 1000\n",
    "            elif pacman_new_loc == ghost_new_loc: # the ghost moved to pacman's new location\n",
    "                return -1000\n",
    "            else:\n",
    "                return 0\n",
    "        \n",
    "        for s in range(num_states):\n",
    "            for pacman_a in range(num_actions):\n",
    "                done = False # flag to signal game has ended\n",
    "                temp = P[s][pacman_a]\n",
    "                pacman_grid_loc = states[s][0] # for the given state, where is pacman\n",
    "                ghost_grid_loc = states[s][1] # in the given state, where is the ghost\n",
    "                \n",
    "                # if pacman performs action a: 0=UP, 1=RIGHT, 2=DOWN, 3=LEFT\n",
    "                [x_p, y_p] = grid_to_xy(pacman_grid_loc)\n",
    "                next_pacman_loc = move(x_p, y_p, pacman_a) # grid location he will move to\n",
    "                \n",
    "                for ghost_a in range(num_actions):\n",
    "                    # if the ghost performs action a: 0=UP, 1=RIGHT, 2=DOWN, 3=LEFT\n",
    "                    [x_g, y_g] = grid_to_xy(ghost_grid_loc)\n",
    "                    next_ghost_loc = move(x_g, y_g, ghost_a) # grid location he will move to\n",
    "                    \n",
    "                    # resulting next state, simulates pacman and the ghost moving simultaneously\n",
    "                    next_state = return_state(states, next_pacman_loc, next_ghost_loc) \n",
    "                    reward = calculate_reward(next_pacman_loc, next_ghost_loc, ghost_grid_loc, pellet_loc) # calculate the reward at this state\n",
    "\n",
    "                    if (pacman_grid_loc == pellet_loc or pacman_grid_loc == ghost_grid_loc):\n",
    "                        done = True\n",
    "\n",
    "                    temp.append( (0.25, next_state, reward, done) )\n",
    "        \n",
    "        self.P = P\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env=PacmanEnv(), gamma=0.9, alpha=0.9, episodes=5):\n",
    "    '''\n",
    "    Q-Learning Algorithm\n",
    "\n",
    "    Inputs:\n",
    "        env: PacmanEnv as defined in class above. This will be used for simplicity of implementation, however in the case\n",
    "            of Q-learning, the state transition probability matrix is assumed to be unknown.\n",
    "        gamma: Discount rate for future rewards.\n",
    "        alpha: Learning rate. \"How much you accept the new value vs the old value,\" i.e. how much weight will you assign\n",
    "            to the old vs new value of Q.\n",
    "        epsilon: Used to control balance of exploration (choose a random action) vs exploitation, i.e. we pick a value at\n",
    "            random in the range (0,1) and if this value < epsilon, we will choose a random action. Else, we pick the action\n",
    "            that maximizes Q (based on current knowledge of Q).\n",
    "        episodes: Number of epochs to run.\n",
    "\n",
    "    Helper Methods:\n",
    "        extract_policy: Returns the optimal policy for a given value function. It is run once at the end of the algorithm\n",
    "                        after the optimal Q (value function) has been estimated.\n",
    "\n",
    "    Outputs:\n",
    "        A tuple (policy, Q, steps) of the policy extracted from the estimated Q function, the approximated optimal value \n",
    "        function, and the number of steps the algorithm took to converge.\n",
    "    '''\n",
    "    \n",
    "    def extract_policy(Q):\n",
    "        policy = np.zeros([env.num_states, env.num_actions])\n",
    "        \n",
    "        for s in range(env.num_states):\n",
    "            best_action = np.argmax(Q[s, :]) # returns index of action that has maximum V\n",
    "            policy[s, best_action] = 1 # deterministic optimal policy, i.e. always take best_action for given state\n",
    "        \n",
    "        return policy\n",
    "    \n",
    "    # initialize Q(s,a) matrix to all zeros\n",
    "    Q = np.zeros([env.num_states, env.num_actions])\n",
    "    steps = 0\n",
    "    \n",
    "    for t in range(episodes):\n",
    "        print('Episode #', t)\n",
    "        converged = False\n",
    "        epsilon = 1/(t+1)\n",
    "        \n",
    "         # select random state\n",
    "        state = random.randint(0, env.num_states-1)\n",
    "        \n",
    "        # run inner loop for each episode until a terminal state has been reached\n",
    "        while not converged:\n",
    "            print('Q learning, step ', steps, '...')\n",
    "            \n",
    "            # select action\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                action = random.randint(0,3) # exploration\n",
    "                #print('random action: ', action)\n",
    "            else:\n",
    "                action = np.argmax(Q[state, :]) # exploitation\n",
    "                #print('exploit action: ', action)\n",
    "\n",
    "            ghost_mvmt = random.randint(0,3) # simulate random movement for the ghost\n",
    "\n",
    "            # travel to the next state, taking action selected above\n",
    "            pacman_grid_loc = env.states[state][0] # for the given state, where is pacman\n",
    "            ghost_grid_loc = env.states[state][1] # in the given state, where is the ghost\n",
    "\n",
    "            [x_p, y_p] = grid_to_xy(pacman_grid_loc)\n",
    "            next_pacman_loc = move(x_p, y_p, action)\n",
    "            [x_g, y_g] = grid_to_xy(ghost_grid_loc)\n",
    "            next_ghost_loc = move(x_g, y_g, ghost_mvmt)\n",
    "\n",
    "            next_state = return_state(env.states, next_pacman_loc, next_ghost_loc)\n",
    "            #print('next state: ', next_state)\n",
    "        \n",
    "            # to get reward, need to find specific entry of P[s][a] with same next_state...\n",
    "            reward = 0\n",
    "            for ns in range( len(env.P[state][action]) ): # array of tuples (probability, next_state, reward, done)\n",
    "                if (env.P[state][action][ns][1] == next_state):\n",
    "                    reward = env.P[state][action][ns][2]\n",
    "            #print('reward: ', reward)\n",
    "\n",
    "            # in next state, select action with highest Q-value\n",
    "            max_next_action_value = np.max(Q[next_state, :])\n",
    "\n",
    "            # update Q-values tables with equation\n",
    "            #print('old value: ', Q[state][action])\n",
    "            Q[state][action] = ((1-alpha)*Q[state][action]) + (alpha*(reward + (gamma * max_next_action_value)))\n",
    "            #print('new value: ', Q[state][action])\n",
    "\n",
    "            # set next state as current state & repeat\n",
    "            state = next_state \n",
    "            steps += 1\n",
    "            \n",
    "            # if reached terminal state (i.e. next state = terminal state), converged = True\n",
    "            #print('convergence: ', env.P[next_state][action][0][3])\n",
    "            converged = env.P[next_state][action][0][3]\n",
    "            #print('converged: ', converged)\n",
    "\n",
    "    # extract optimal policy after calculating optimal V\n",
    "    policy = extract_policy(Q)\n",
    "\n",
    "    return policy, Q, steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode # 0\n",
      "Q learning, step  0 ...\n",
      "Q learning, step  1 ...\n",
      "Q learning, step  2 ...\n",
      "Q learning, step  3 ...\n",
      "Q learning, step  4 ...\n",
      "Q learning, step  5 ...\n",
      "Q learning, step  6 ...\n",
      "Q learning, step  7 ...\n",
      "Q learning, step  8 ...\n",
      "Q learning, step  9 ...\n",
      "Q learning, step  10 ...\n",
      "Q learning, step  11 ...\n",
      "Q learning, step  12 ...\n",
      "Q learning, step  13 ...\n",
      "Q learning, step  14 ...\n",
      "Q learning, step  15 ...\n",
      "Q learning, step  16 ...\n",
      "Q learning, step  17 ...\n",
      "Q learning, step  18 ...\n",
      "Q learning, step  19 ...\n",
      "Q learning, step  20 ...\n",
      "Q learning, step  21 ...\n",
      "Q learning, step  22 ...\n",
      "Q learning, step  23 ...\n",
      "Q learning, step  24 ...\n",
      "Q learning, step  25 ...\n",
      "Q learning, step  26 ...\n",
      "Q learning, step  27 ...\n",
      "Q learning, step  28 ...\n",
      "Episode # 1\n",
      "Q learning, step  29 ...\n",
      "Q learning, step  30 ...\n",
      "Episode # 2\n",
      "Q learning, step  31 ...\n",
      "Q learning, step  32 ...\n",
      "Q learning, step  33 ...\n",
      "Q learning, step  34 ...\n",
      "Q learning, step  35 ...\n",
      "Q learning, step  36 ...\n",
      "Episode # 3\n",
      "Q learning, step  37 ...\n",
      "Episode # 4\n",
      "Q learning, step  38 ...\n",
      "Q learning, step  39 ...\n",
      "Q learning, step  40 ...\n",
      "Q learning, step  41 ...\n",
      "Q learning, step  42 ...\n",
      "Q learning, step  43 ...\n",
      "Q learning, step  44 ...\n",
      "Q learning, step  45 ...\n",
      "Q learning, step  46 ...\n",
      "Q learning, step  47 ...\n",
      "Q learning, step  48 ...\n",
      "Q learning, step  49 ...\n",
      "Q learning, step  50 ...\n",
      "Q learning, step  51 ...\n",
      "Q learning, step  52 ...\n"
     ]
    }
   ],
   "source": [
    "policy, Q, steps = q_learning()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
