{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "UP = 0\n",
    "RIGHT = 1\n",
    "DOWN = 2\n",
    "LEFT = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Enumerate states for a 3x3 grid ==> 81 states \n",
    "(9 choices for pacman location x 9 choices for ghost location)\n",
    "\n",
    "y    _0_|_1_|_2_\n",
    "|    _3_|_4_|_5_\n",
    "v     6 | 7 | 8\n",
    "    x -->\n",
    "\n",
    "Each x,y pair represented as an integer number corresponding to the diagram above\n",
    "'''\n",
    "\n",
    "states_num = [];\n",
    "\n",
    "for s in range(81):\n",
    "    for p in range(9):\n",
    "        for g in range(9):\n",
    "            states_num.append( (p, g) )\n",
    "                    \n",
    "#for s in range(81):\n",
    "#    print(\"state \", s, \": \", states_num[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_to_xy(number):\n",
    "    switch = {\n",
    "        0: [0,0],\n",
    "        1: [1,0],\n",
    "        2: [2,0],\n",
    "        3: [0,1],\n",
    "        4: [1,1],\n",
    "        5: [2,1],\n",
    "        6: [0,2],\n",
    "        7: [1,2],\n",
    "        8: [2,2]\n",
    "    }\n",
    "    return switch.get(number, \"invalid entry\")\n",
    "\n",
    "def xy_to_grid(x,y):\n",
    "    switch = {\n",
    "        0: {0:0, 1:3, 2:6},\n",
    "        1: {0:1, 1:4, 2:7},\n",
    "        2: {0:2, 1:5, 2:8}\n",
    "    }\n",
    "    x = switch.get(x,\"invalid entry\")\n",
    "\n",
    "    if x == \"invalid entry\":\n",
    "        return x\n",
    "    else:\n",
    "        return x.get(y,\"invalid entry\")\n",
    "\n",
    "def return_state(states, p, g):\n",
    "    return states.index( (p,g) )\n",
    "\n",
    "def move(x, y, action):\n",
    "    if action == UP:\n",
    "        y = max(0, y-1)\n",
    "    elif action == RIGHT:\n",
    "        x = min(2, x+1)\n",
    "    elif action == DOWN:\n",
    "        y = min(2, y+1)\n",
    "    elif action == LEFT:\n",
    "        x = max(0, x-1)\n",
    "    return xy_to_grid(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PacmanEnv:\n",
    "    '''\n",
    "    Class to initialize and store information about the Pacman environment for program planning algorithms.\n",
    "\n",
    "    Properties:\n",
    "        P[s][a] is a list of is a list of transition tuples (prob, next_state, reward, done)\n",
    "        num_states = number of states (set to default for 3x3 grid)\n",
    "        num_actions = number of actions (set to 4)\n",
    "        pellet_loc = location of pellet (set to 2, i.e. [2,0] by default)\n",
    "\n",
    "    Methods:\n",
    "        return_state: Returns state number given location of pacman and the ghost\n",
    "        move: Moves pacman given current location and action input. Returns grid location number\n",
    "        calculate_reward: Returns reward for current location of pacman. Used to evaluate R(s,a,s') by \n",
    "                        first determining s' through move(s,a), then calculating the reward at s'.\n",
    "        grid_to_xy: Returns corresponding (x,y) coordinate pair for valid grid location integer input\n",
    "                    If number out of range, returns 'invalid entry' error message\n",
    "        xy_to_grid: Returns corresponding grid location # for given (x,y) coordinate pair input\n",
    "                    If number out of range, returns 'invalid entry' error message\n",
    "    '''\n",
    "\n",
    "    def __init__(self, states=states_num, num_states=81, num_actions=4, pellet_loc=2):\n",
    "        self.states = states\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.pellet_loc = pellet_loc\n",
    "        \n",
    "        P = {s : {a : [] for a in range(num_actions)} for s in range(num_states)}\n",
    "        \n",
    "        # parameters must be of the same type, i.e. [x,y] or int value 0-8\n",
    "        # need to adjust to include reward definition for bumping into walls\n",
    "        def calculate_reward(pacman_new_loc, ghost_new_loc, ghost_current_loc, pellet_location):\n",
    "            if pacman_new_loc == ghost_current_loc: # pacman moved to the ghost's location\n",
    "                return -1000\n",
    "            elif pacman_new_loc == pellet_location:\n",
    "                return 1000\n",
    "            elif pacman_new_loc == ghost_new_loc: # the ghost moved to pacman's new location\n",
    "                return -1000\n",
    "            else:\n",
    "                return 0\n",
    "        \n",
    "        for s in range(num_states):\n",
    "            for pacman_a in range(num_actions):\n",
    "                done = False # flag to signal game has ended\n",
    "                temp = P[s][pacman_a]\n",
    "                pacman_grid_loc = states[s][0] # for the given state, where is pacman\n",
    "                ghost_grid_loc = states[s][1] # in the given state, where is the ghost\n",
    "                \n",
    "                # if pacman performs action a: 0=UP, 1=RIGHT, 2=DOWN, 3=LEFT\n",
    "                [x_p, y_p] = grid_to_xy(pacman_grid_loc)\n",
    "                next_pacman_loc = move(x_p, y_p, pacman_a) # grid location he will move to\n",
    "                \n",
    "                for ghost_a in range(num_actions):\n",
    "                    # if the ghost performs action a: 0=UP, 1=RIGHT, 2=DOWN, 3=LEFT\n",
    "                    [x_g, y_g] = grid_to_xy(ghost_grid_loc)\n",
    "                    next_ghost_loc = move(x_g, y_g, ghost_a) # grid location he will move to\n",
    "                    \n",
    "                    # resulting next state, simulates pacman and the ghost moving simultaneously\n",
    "                    next_state = return_state(states, next_pacman_loc, next_ghost_loc) \n",
    "                    reward = calculate_reward(next_pacman_loc, next_ghost_loc, ghost_grid_loc, pellet_loc) # calculate the reward at this state\n",
    "\n",
    "                    if (pacman_grid_loc == pellet_loc or pacman_grid_loc == ghost_grid_loc):\n",
    "                        done = True\n",
    "\n",
    "                    temp.append( (0.25, next_state, reward, done) )\n",
    "        \n",
    "        self.P = P\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env=PacmanEnv(), gamma=0.9, theta=1e-5):\n",
    "    '''\n",
    "    Value Iteration Algorithm\n",
    "\n",
    "    Inputs:\n",
    "        env: PacmanEnv as defined in class above.\n",
    "        gamma: Discount rate for future rewards.\n",
    "        theta: Stopping criterion value. When change in Value function is less than theta for every state, stop.\n",
    "\n",
    "    Helper Methods:\n",
    "        calculate_action_values: Calculates the values for all actions for a given state.\n",
    "                                Returns a vector action_values of length num_actions, where \n",
    "                                action_values[a] = expected value of action a.\n",
    "                                The expected value is calculated according to the Bellman equation:\n",
    "                                V(s) = P(s'|s,a) * ( R(s,a) + (gamma * V(s')) )\n",
    "        extract_policy: Returns the optimal policy for a given value function. It is run once at the end of the algorithm\n",
    "                        after the optimal V (value function) has been calculated.\n",
    "\n",
    "    Outputs:\n",
    "        A tuple (policy, V, steps) of the optimal policy, the approximated optimal value function, and the number of steps\n",
    "        the algorithm took to converge.\n",
    "    '''\n",
    "    \n",
    "    def calculate_action_values(current_state, V):\n",
    "        action_values = np.zeros(env.num_actions)\n",
    "        for a in range(env.num_actions):\n",
    "            for prob, next_state, reward, done in env.P[current_state][a]:\n",
    "                action_values[a] += prob * (reward + (gamma * V[next_state]))\n",
    "        return action_values\n",
    "    \n",
    "    def extract_policy(V):\n",
    "        policy = np.zeros([env.num_states, env.num_actions])\n",
    "        \n",
    "        for s in range(env.num_states):\n",
    "            action_values = calculate_action_values(s, V)\n",
    "            best_action = np.argmax(action_values) # returns index of action that has maximum V\n",
    "            policy[s, best_action] = 1 # deterministic optimal policy, i.e. always take best_action for given state\n",
    "        \n",
    "        return policy\n",
    "    \n",
    "    V = np.zeros(env.num_states) # arbitrarily initialize vector V to be all zeros\n",
    "    converged = False\n",
    "    steps = 0\n",
    "    \n",
    "    # iteratively calculate optimal V\n",
    "    while not converged:\n",
    "        print('Value iteration, step ', steps, '...')\n",
    "        delta = 0\n",
    "        for s in range(env.num_states):\n",
    "            action_values = calculate_action_values(s, V)\n",
    "            max_action_value = np.max(action_values)\n",
    "            delta = max( delta, np.abs(max_action_value - V[s]) ) # the maximum difference between V'(s) and V(s) for all s\n",
    "            V[s] = max_action_value        \n",
    "        \n",
    "        steps += 1\n",
    "        \n",
    "        #print('Delta: ', delta)\n",
    "        converged = (delta < theta)\n",
    "        #print(converged)\n",
    "    \n",
    "    # extract optimal policy after calculating optimal V\n",
    "    policy = extract_policy(V)\n",
    "    \n",
    "    return policy, V, steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "winReward = 1000\n",
    "loseReward = -1000\n",
    "\n",
    "def nextMove(env, currState, action):\n",
    "    '''\n",
    "    Returns all possible next states given action, and the reward given next states.\n",
    "    ARGS:\n",
    "        currState: Current game state, ranges from 0 to numStates-1.\n",
    "        action: Action to perform.\n",
    "    RETURN:\n",
    "        nextStates: List of possible next states give current state and action.\n",
    "                    Largly depends on potential ghost movements.\n",
    "        rewards: List of rewards for each potential next state.\n",
    "    '''\n",
    "    nextStates = [];\n",
    "    # Get currState coordinates\n",
    "    \n",
    "    pacman_grid_loc = env.states[currState][0] # for the given state, where is pacman\n",
    "    ghost_grid_loc = env.states[currState][1] # in the given state, where is the ghost\n",
    "                \n",
    "    [x_p, y_p] = grid_to_xy(pacman_grid_loc)\n",
    "    [x_g, y_g] = grid_to_xy(ghost_grid_loc)\n",
    "    \n",
    "    # Get pacman location after performing action\n",
    "    next_pacman_loc = move(x_p, y_p, action)\n",
    "    pacmanLocX_next, pacmanLocY_next = grid_to_xy(next_pacman_loc)\n",
    "    \n",
    "    # Initialize rewards\n",
    "    if (next_pacman_loc == env.pellet_loc): \n",
    "        rewards = [winReward]*4;\n",
    "    elif (next_pacman_loc == ghost_grid_loc):\n",
    "        rewards = [loseReward]*4;\n",
    "    else:\n",
    "        rewards = [0]*4;\n",
    "\n",
    "    # Iterate through possible ghost states\n",
    "    for ghostAction in range(4):\n",
    "        next_ghost_loc = move(x_g, y_g, ghostAction)\n",
    "        ghostLocX_next, ghostLocY_next = grid_to_xy(next_ghost_loc)\n",
    "        state = return_state(env.states, next_pacman_loc, next_ghost_loc)\n",
    "        nextStates.append(state);     \n",
    "        # Evaluate reward - pacman eaten by ghost\n",
    "        if (next_pacman_loc == next_ghost_loc):\n",
    "            rewards[ghostAction] = loseReward;\n",
    "\n",
    "    return nextStates, rewards;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "winReward = 1000\n",
    "loseReward = -1000\n",
    "\n",
    "###############################################################################\n",
    "############################  Policy Iteration ################################\n",
    "###############################################################################\n",
    "def policy_iteration(env=PacmanEnv(), deltaLim=1e-5, gamma=0.9):\n",
    "    '''\n",
    "    Policy iteration algorithm consists of two steps that are repeated in each iteration of the loop:\n",
    "    policy evaluation (calculate V) and policy improvement (greedy approach to improve current policy).\n",
    "    \n",
    "    Helper Methods:\n",
    "        policyEvaluation\n",
    "        policyImprovement\n",
    "    '''\n",
    "    prob = 0.25\n",
    "    \n",
    "    ########################## Policy evaluation ##################################\n",
    "    def policyEvaluation(policy, v, deltaLim=1e-5, delta=10000):\n",
    "        cnt = 0;\n",
    "        while (delta > deltaLim): \n",
    "            delta = 0;\n",
    "            for state in range(env.num_states):\n",
    "                if (env.P[state][0][0][3] == True): # game has ended\n",
    "                    v[state] = 0;\n",
    "                    continue;\n",
    "                vStateOld = v[state]; \n",
    "                # Choose action from policy\n",
    "                action = policy[state];\n",
    "                # Potential next states given current state and action\n",
    "                nextStates, rewards = nextMove(env, state, action); \n",
    "                # Compute V(s)\n",
    "                vNextStates = [prob*(rewards[i]+gamma*v[nextStates[i]]) for i in range(len(nextStates))];\n",
    "                v[state] = sum(vNextStates);\n",
    "\n",
    "                delta = max(delta, abs(v[state]-vStateOld));\n",
    "            print(\"Delta\",delta);\n",
    "            cnt += 1;\n",
    "        print(\"Policy evalulation converged at\", cnt);\n",
    "        return v;\n",
    "\n",
    "\n",
    "    ########################## Policy Improvement #################################\n",
    "    def policyImprovement(policy, v):\n",
    "        policyStable = True;\n",
    "        for state in range(env.num_states):\n",
    "            if (env.P[state][0][0][3] == True): # game ended\n",
    "                continue;\n",
    "            oldAction = policy[state];\n",
    "            # Optimal action and V(s,a)\n",
    "            optAction = 0; \n",
    "            optActionVal = None;\n",
    "            for action in range(4):\n",
    "                # Potential next states given current state and action\n",
    "                nextStates, rewards = nextMove(env, state, action);\n",
    "                # Compute V(s,a)\n",
    "                vNextStates = [prob*(rewards[i]+gamma*v[nextStates[i]]) for i in range(len(nextStates))];\n",
    "                actionVal = sum(vNextStates);\n",
    "                if (optActionVal == None):\n",
    "                    optActionVal = actionVal;\n",
    "                if (actionVal > optActionVal): # Choose action with largest V(s,a)\n",
    "                    optActionVal = actionVal;\n",
    "                    optAction = action;\n",
    "            # No convergence\n",
    "            if (oldAction != optAction):\n",
    "                policyStable = False;\n",
    "                policy[state] = optAction;\n",
    "        return policy, policyStable\n",
    "    \n",
    "    # Init\n",
    "    v = [0]*env.num_states;\n",
    "    policy = [0]*env.num_states;\n",
    "    policyStable = False;\n",
    "\n",
    "    # Run Policy Iteration\n",
    "    policyIter = 1;\n",
    "    while (not policyStable):\n",
    "        v = policyEvaluation(policy, v);\n",
    "        policy, policyStable = policyImprovement(policy, v);\n",
    "        # print(\"Total iterations:\", policyIter)\n",
    "        policyIter += 1;\n",
    "\n",
    "    return policy, v, policyIter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta 1345.9564041748047\n",
      "Delta 138.45322265624998\n",
      "Delta 80.36209533691408\n",
      "Delta 50.327251831054696\n",
      "Delta 33.73082389033124\n",
      "Delta 27.188297693384726\n",
      "Delta 21.966299296992872\n",
      "Delta 17.57000472180266\n",
      "Delta 13.958241635996899\n",
      "Delta 11.036803885005725\n",
      "Delta 8.69804945493371\n",
      "Delta 6.838899352821443\n",
      "Delta 5.368187924993606\n",
      "Delta 4.20872259239178\n",
      "Delta 3.296839179868016\n",
      "Delta 2.5809058406553618\n",
      "Delta 2.0195120096609003\n",
      "Delta 1.5796944540835511\n",
      "Delta 1.2353503818944773\n",
      "Delta 0.9658848837300411\n",
      "Delta 0.7550907005195313\n",
      "Delta 0.5902370677873137\n",
      "Delta 0.461337393511144\n",
      "Delta 0.36056539611337257\n",
      "Delta 0.28325655650687054\n",
      "Delta 0.22723088283598258\n",
      "Delta 0.18228629704020705\n",
      "Delta 0.1462312642899235\n",
      "Delta 0.11730760240138238\n",
      "Delta 0.09410482672890907\n",
      "Delta 0.0754914077958233\n",
      "Delta 0.06055961086141792\n",
      "Delta 0.04858123833491845\n",
      "Delta 0.038972122246548224\n",
      "Delta 0.0312636379597393\n",
      "Delta 0.025079851325187974\n",
      "Delta 0.020119185502721848\n",
      "Delta 0.01613971373265599\n",
      "Delta 0.012947360991546475\n",
      "Delta 0.010386439231950817\n",
      "Delta 0.008332054670262323\n",
      "Delta 0.0066840168564681335\n",
      "Delta 0.005361952490204658\n",
      "Delta 0.004301385694219562\n",
      "Delta 0.003450593588098627\n",
      "Delta 0.002768083811389488\n",
      "Delta 0.0022205709799720807\n",
      "Delta 0.0017813533883099808\n",
      "Delta 0.001429010791582641\n",
      "Delta 0.001146359759815141\n",
      "Delta 0.0009196156576081194\n",
      "Delta 0.0007377203801013366\n",
      "Delta 0.0005918030589100454\n",
      "Delta 0.00047474743817588205\n",
      "Delta 0.00038084482102362927\n",
      "Delta 0.0003055156616653676\n",
      "Delta 0.0002450862250498176\n",
      "Delta 0.00019660942220411926\n",
      "Delta 0.00015772108326927992\n",
      "Delta 0.00012652465898099763\n",
      "Delta 0.00010149872804277038\n",
      "Delta 8.14227983028104e-05\n",
      "Delta 6.531778490170836e-05\n",
      "Delta 5.239826083425214e-05\n",
      "Delta 4.203415261372356e-05\n",
      "Delta 3.3720012027060875e-05\n",
      "Delta 2.7050365986269753e-05\n",
      "Delta 2.1699941839870007e-05\n",
      "Delta 1.740780419368093e-05\n",
      "Delta 1.3964629516749483e-05\n",
      "Delta 1.12024971201663e-05\n",
      "Delta 8.986700493096578e-06\n",
      "Policy evalulation converged at 72\n",
      "Delta 1983.52447518561\n",
      "Delta 1373.4239493628654\n",
      "Delta 367.97820436589495\n",
      "Delta 81.99281029329609\n",
      "Delta 46.1936225101141\n",
      "Delta 33.29889523140966\n",
      "Delta 25.49708716710242\n",
      "Delta 19.920871553053928\n",
      "Delta 15.333221545589339\n",
      "Delta 11.691198216743302\n",
      "Delta 8.858606780645118\n",
      "Delta 6.683623058582455\n",
      "Delta 5.0276199197219285\n",
      "Delta 3.7739793963778823\n",
      "Delta 2.8287096201825648\n",
      "Delta 2.1179471503654668\n",
      "Delta 1.5845703822127604\n",
      "Delta 1.184871773773061\n",
      "Delta 0.8856487212062945\n",
      "Delta 0.6618047164758352\n",
      "Delta 0.49443675239351137\n",
      "Delta 0.3693420725802241\n",
      "Delta 0.2758682022232648\n",
      "Delta 0.20603548838224128\n",
      "Delta 0.15387181171104203\n",
      "Delta 0.11491039337865061\n",
      "Delta 0.08581189241800757\n",
      "Delta 0.06408065804021845\n",
      "Delta 0.04785201162363251\n",
      "Delta 0.03573295590677361\n",
      "Delta 0.02668298768877264\n",
      "Delta 0.019924968516491504\n",
      "Delta 0.014878500364595482\n",
      "Delta 0.011110138514830226\n",
      "Delta 0.00829619443778995\n",
      "Delta 0.006194949185584164\n",
      "Delta 0.0046258987962986\n",
      "Delta 0.003454253323248224\n",
      "Delta 0.0025793602896726497\n",
      "Delta 0.001926059362126864\n",
      "Delta 0.0014382262368926035\n",
      "Delta 0.0010739514776787473\n",
      "Delta 0.0008019403221837251\n",
      "Delta 0.0005988242743484307\n",
      "Delta 0.0004471535788113101\n",
      "Delta 0.00033389814173290233\n",
      "Delta 0.00024932812813460714\n",
      "Delta 0.00018617807648979579\n",
      "Delta 0.0001390227237578756\n",
      "Delta 0.00010381092020850247\n",
      "Delta 7.751759390828283e-05\n",
      "Delta 5.788386529559375e-05\n",
      "Delta 4.32229855888977e-05\n",
      "Delta 3.227542712380682e-05\n",
      "Delta 2.4100676498051143e-05\n",
      "Delta 1.799643438005205e-05\n",
      "Delta 1.3438280454636242e-05\n",
      "Delta 1.0034620082421952e-05\n",
      "Delta 7.4930419913243895e-06\n",
      "Policy evalulation converged at 59\n",
      "Delta 575.6933578158303\n",
      "Delta 281.0692731783987\n",
      "Delta 159.42022764983787\n",
      "Delta 68.76997443373875\n",
      "Delta 42.11357831627197\n",
      "Delta 29.416685029819405\n",
      "Delta 18.819129904932424\n",
      "Delta 11.826635346440582\n",
      "Delta 7.40006660784087\n",
      "Delta 4.621312631672367\n",
      "Delta 2.8827750052125793\n",
      "Delta 1.7972859970449804\n",
      "Delta 1.1202555482954608\n",
      "Delta 0.6981882398160906\n",
      "Delta 0.4351213760144219\n",
      "Delta 0.2711701149658552\n",
      "Delta 0.1689938971888978\n",
      "Delta 0.10531725997725516\n",
      "Delta 0.0656338568717274\n",
      "Delta 0.0409031061306564\n",
      "Delta 0.025490870775342955\n",
      "Delta 0.015885945606441965\n",
      "Delta 0.009900143330867195\n",
      "Delta 0.00616978313968275\n",
      "Delta 0.003845017490675673\n",
      "Delta 0.002396220291643658\n",
      "Delta 0.0014933278481521484\n",
      "Delta 0.0009306440111345182\n",
      "Delta 0.000579978654172919\n",
      "Delta 0.0003614435115650849\n",
      "Delta 0.00022525210385992978\n",
      "Delta 0.00014037742738537418\n",
      "Delta 8.748340997044579e-05\n",
      "Delta 5.451978375958788e-05\n",
      "Delta 3.397680580974338e-05\n",
      "Delta 2.1174393225464883e-05\n",
      "Delta 1.3195911435559537e-05\n",
      "Delta 8.223710551646946e-06\n",
      "Policy evalulation converged at 38\n",
      "Delta 324.9487919489445\n",
      "Delta 32.590584892919196\n",
      "Delta 12.186538746967472\n",
      "Delta 6.510772064518051\n",
      "Delta 4.345290450975199\n",
      "Delta 2.7265343376084274\n",
      "Delta 1.6427738636182312\n",
      "Delta 0.9641182941177249\n",
      "Delta 0.5558989538719743\n",
      "Delta 0.3166068627710956\n",
      "Delta 0.17875650024996048\n",
      "Delta 0.10029652141793122\n",
      "Delta 0.05601919088746854\n",
      "Delta 0.0311848505288026\n",
      "Delta 0.017317575060474155\n",
      "Delta 0.00959939371443852\n",
      "Delta 0.005313938729273104\n",
      "Delta 0.0029386957733095187\n",
      "Delta 0.001623934733288479\n",
      "Delta 0.0008968925639010195\n",
      "Delta 0.000495143710281809\n",
      "Delta 0.0002732666650899773\n",
      "Delta 0.0001507789172592311\n",
      "Delta 8.317993945183844e-05\n",
      "Delta 4.588171441355371e-05\n",
      "Delta 2.530567860503652e-05\n",
      "Delta 1.3956108432466863e-05\n",
      "Delta 7.696383249822247e-06\n",
      "Policy evalulation converged at 28\n",
      "Delta 11.130497036543602\n",
      "Delta 2.5043620515442626\n",
      "Delta 1.2934082178617246e-06\n",
      "Policy evalulation converged at 3\n"
     ]
    }
   ],
   "source": [
    "policy, V, steps = policy_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value iteration, step  0 ...\n",
      "Value iteration, step  1 ...\n",
      "Value iteration, step  2 ...\n",
      "Value iteration, step  3 ...\n",
      "Value iteration, step  4 ...\n",
      "Value iteration, step  5 ...\n",
      "Value iteration, step  6 ...\n",
      "Value iteration, step  7 ...\n",
      "Value iteration, step  8 ...\n",
      "Value iteration, step  9 ...\n",
      "Value iteration, step  10 ...\n",
      "Value iteration, step  11 ...\n",
      "Value iteration, step  12 ...\n",
      "Value iteration, step  13 ...\n",
      "Value iteration, step  14 ...\n",
      "Value iteration, step  15 ...\n",
      "Value iteration, step  16 ...\n",
      "Value iteration, step  17 ...\n",
      "Value iteration, step  18 ...\n",
      "Value iteration, step  19 ...\n",
      "Value iteration, step  20 ...\n",
      "-----------------------------------------\n",
      "Optimal extracted policy: \n",
      "[[0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "-----------------------------------------\n",
      "\n",
      "Location of Pacman:  [0, 0]\n",
      "Location of Ghost:  [0, 0]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [0, 0]\n",
      "Location of Ghost:  [1, 0]\n",
      "Best action determined by optimal policy:  DOWN\n",
      "\n",
      "Location of Pacman:  [0, 0]\n",
      "Location of Ghost:  [2, 0]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [0, 0]\n",
      "Location of Ghost:  [0, 1]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [0, 0]\n",
      "Location of Ghost:  [1, 1]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [0, 0]\n",
      "Location of Ghost:  [2, 1]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [0, 0]\n",
      "Location of Ghost:  [0, 2]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [0, 0]\n",
      "Location of Ghost:  [1, 2]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [0, 0]\n",
      "Location of Ghost:  [2, 2]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [1, 0]\n",
      "Location of Ghost:  [0, 0]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [1, 0]\n",
      "Location of Ghost:  [1, 0]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [1, 0]\n",
      "Location of Ghost:  [2, 0]\n",
      "Best action determined by optimal policy:  DOWN\n",
      "\n",
      "Location of Pacman:  [1, 0]\n",
      "Location of Ghost:  [0, 1]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [1, 0]\n",
      "Location of Ghost:  [1, 1]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [1, 0]\n",
      "Location of Ghost:  [2, 1]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [1, 0]\n",
      "Location of Ghost:  [0, 2]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [1, 0]\n",
      "Location of Ghost:  [1, 2]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [1, 0]\n",
      "Location of Ghost:  [2, 2]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [2, 0]\n",
      "Location of Ghost:  [0, 0]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [2, 0]\n",
      "Location of Ghost:  [1, 0]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [2, 0]\n",
      "Location of Ghost:  [2, 0]\n",
      "Best action determined by optimal policy:  DOWN\n",
      "\n",
      "Location of Pacman:  [2, 0]\n",
      "Location of Ghost:  [0, 1]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [2, 0]\n",
      "Location of Ghost:  [1, 1]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [2, 0]\n",
      "Location of Ghost:  [2, 1]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [2, 0]\n",
      "Location of Ghost:  [0, 2]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [2, 0]\n",
      "Location of Ghost:  [1, 2]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [2, 0]\n",
      "Location of Ghost:  [2, 2]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [0, 1]\n",
      "Location of Ghost:  [0, 0]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [0, 1]\n",
      "Location of Ghost:  [1, 0]\n",
      "Best action determined by optimal policy:  LEFT\n",
      "\n",
      "Location of Pacman:  [0, 1]\n",
      "Location of Ghost:  [2, 0]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [0, 1]\n",
      "Location of Ghost:  [0, 1]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [0, 1]\n",
      "Location of Ghost:  [1, 1]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [0, 1]\n",
      "Location of Ghost:  [2, 1]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [0, 1]\n",
      "Location of Ghost:  [0, 2]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [0, 1]\n",
      "Location of Ghost:  [1, 2]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [0, 1]\n",
      "Location of Ghost:  [2, 2]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [1, 1]\n",
      "Location of Ghost:  [0, 0]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [1, 1]\n",
      "Location of Ghost:  [1, 0]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [1, 1]\n",
      "Location of Ghost:  [2, 0]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [1, 1]\n",
      "Location of Ghost:  [0, 1]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [1, 1]\n",
      "Location of Ghost:  [1, 1]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [1, 1]\n",
      "Location of Ghost:  [2, 1]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [1, 1]\n",
      "Location of Ghost:  [0, 2]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [1, 1]\n",
      "Location of Ghost:  [1, 2]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [1, 1]\n",
      "Location of Ghost:  [2, 2]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [2, 1]\n",
      "Location of Ghost:  [0, 0]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [2, 1]\n",
      "Location of Ghost:  [1, 0]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [2, 1]\n",
      "Location of Ghost:  [2, 0]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [2, 1]\n",
      "Location of Ghost:  [0, 1]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [2, 1]\n",
      "Location of Ghost:  [1, 1]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [2, 1]\n",
      "Location of Ghost:  [2, 1]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [2, 1]\n",
      "Location of Ghost:  [0, 2]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [2, 1]\n",
      "Location of Ghost:  [1, 2]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [2, 1]\n",
      "Location of Ghost:  [2, 2]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [0, 2]\n",
      "Location of Ghost:  [0, 0]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [0, 2]\n",
      "Location of Ghost:  [1, 0]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [0, 2]\n",
      "Location of Ghost:  [2, 0]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [0, 2]\n",
      "Location of Ghost:  [0, 1]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [0, 2]\n",
      "Location of Ghost:  [1, 1]\n",
      "Best action determined by optimal policy:  DOWN\n",
      "\n",
      "Location of Pacman:  [0, 2]\n",
      "Location of Ghost:  [2, 1]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [0, 2]\n",
      "Location of Ghost:  [0, 2]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [0, 2]\n",
      "Location of Ghost:  [1, 2]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [0, 2]\n",
      "Location of Ghost:  [2, 2]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [1, 2]\n",
      "Location of Ghost:  [0, 0]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [1, 2]\n",
      "Location of Ghost:  [1, 0]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [1, 2]\n",
      "Location of Ghost:  [2, 0]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [1, 2]\n",
      "Location of Ghost:  [0, 1]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [1, 2]\n",
      "Location of Ghost:  [1, 1]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [1, 2]\n",
      "Location of Ghost:  [2, 1]\n",
      "Best action determined by optimal policy:  DOWN\n",
      "\n",
      "Location of Pacman:  [1, 2]\n",
      "Location of Ghost:  [0, 2]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [1, 2]\n",
      "Location of Ghost:  [1, 2]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [1, 2]\n",
      "Location of Ghost:  [2, 2]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [2, 2]\n",
      "Location of Ghost:  [0, 0]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [2, 2]\n",
      "Location of Ghost:  [1, 0]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [2, 2]\n",
      "Location of Ghost:  [2, 0]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [2, 2]\n",
      "Location of Ghost:  [0, 1]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [2, 2]\n",
      "Location of Ghost:  [1, 1]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [2, 2]\n",
      "Location of Ghost:  [2, 1]\n",
      "Best action determined by optimal policy:  LEFT\n",
      "\n",
      "Location of Pacman:  [2, 2]\n",
      "Location of Ghost:  [0, 2]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [2, 2]\n",
      "Location of Ghost:  [1, 2]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [2, 2]\n",
      "Location of Ghost:  [2, 2]\n",
      "Best action determined by optimal policy:  UP\n",
      "-----------------------------------------\n",
      "Value Function: \n",
      "[ 702.38094709  150.32467305  244.04761686  982.14285284  678.5714255\n",
      "  732.14285531  994.04761561  982.14285471  952.38095078 1952.38094776\n",
      " 1732.14285429  244.04761781 1982.14285368 1928.57142654 1732.14285604\n",
      " 1994.04761654 1982.14285556 1952.38095139 1952.38094776 1732.14285529\n",
      "  244.04761824 1982.14285467 1928.57142748 1732.14285645 1994.04761708\n",
      " 1982.14285623 1952.38095174  452.38095006  150.32467355  244.04761781\n",
      "  200.89285513  355.84415449  325.89285606  494.04761735  450.89285583\n",
      "  452.38095145  952.38095044  732.1428558   244.04761824  982.14285557\n",
      "  678.57142745  732.14285647  994.04761779  982.14285625  952.3809518\n",
      " 1952.38095069 1732.1428561   244.0476186  1982.14285587 1928.57142783\n",
      " 1732.14285674 1994.04761813 1982.14285656 1952.38095202  210.19345133\n",
      "  172.2706972   120.53909566  219.14569711   97.85409838  172.27069748\n",
      "  -45.01488179  219.14569739  210.19345185  452.38095153  325.89285649\n",
      "  244.0476186   450.8928564   355.8441553   150.32467496  494.04761843\n",
      "  200.89285666  452.38095204  952.38095167  732.14285665  244.04761875\n",
      "  982.14285656  678.57142816  150.32467511  994.04761859  982.14285682\n",
      "  702.38095217]\n",
      "Steps 'til convergence:  21\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "### Value Iteration Result ###\n",
    "def convert_to_action(num):\n",
    "    switch = {\n",
    "                0: 'UP',\n",
    "                1: 'RIGHT',\n",
    "                2: 'DOWN',\n",
    "                3: 'LEFT'\n",
    "            }\n",
    "    return switch.get(num, \"invalid entry\")\n",
    "\n",
    "policy, V, steps = value_iteration(gamma=0.5)\n",
    "env = PacmanEnv()\n",
    "action_list = []\n",
    "\n",
    "print('-----------------------------------------')\n",
    "print('Optimal extracted policy: ')\n",
    "print(policy)\n",
    "\n",
    "print('-----------------------------------------')\n",
    "for s in range(env.num_states):\n",
    "    print()\n",
    "    print('Location of Pacman: ', grid_to_xy(env.states[s][0]))\n",
    "    print('Location of Ghost: ', grid_to_xy(env.states[s][1]))\n",
    "    print('Best action determined by optimal policy: ', convert_to_action( policy[s].tolist().index(1) ) )\n",
    "    action_list.append(convert_to_action( policy[s].tolist().index(1) ))\n",
    "\n",
    "print('-----------------------------------------')    \n",
    "print('Value Function: ')\n",
    "print(V)\n",
    "\n",
    "print('Steps \\'til convergence: ', steps)\n",
    "print('-----------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta 1158.9641571044922\n",
      "Delta 70.37353515625\n",
      "Delta 22.15576171875\n",
      "Delta 7.0171356201171875\n",
      "Delta 2.2110939025878906\n",
      "Delta 0.7046712562441826\n",
      "Delta 0.2284294314449653\n",
      "Delta 0.0753918632199202\n",
      "Delta 0.025302088840135184\n",
      "Delta 0.008614999952101243\n",
      "Delta 0.0029681235470206957\n",
      "Delta 0.0010321105410753262\n",
      "Delta 0.0003614107556018098\n",
      "Delta 0.0001271984433515172\n",
      "Delta 4.4927232920599636e-05\n",
      "Delta 1.5906580870250764e-05\n",
      "Delta 5.640310405397031e-06\n",
      "Policy evalulation converged at 17\n",
      "Delta 1434.5930225674501\n",
      "Delta 593.0232546375245\n",
      "Delta 83.82783805393524\n",
      "Delta 9.83109652231062\n",
      "Delta 2.2626809498884084\n",
      "Delta 0.7660799726641727\n",
      "Delta 0.25537548775460905\n",
      "Delta 0.08474518926699659\n",
      "Delta 0.02807864839445351\n",
      "Delta 0.009298309990644071\n",
      "Delta 0.003078785205561374\n",
      "Delta 0.0010195006612541135\n",
      "Delta 0.00033765196742407966\n",
      "Delta 0.00011185193935148163\n",
      "Delta 3.7061030845109144e-05\n",
      "Delta 1.2282651917772114e-05\n",
      "Delta 4.071596892529783e-06\n",
      "Policy evalulation converged at 17\n",
      "Delta 420.77652488751005\n",
      "Delta 116.01761708542676\n",
      "Delta 11.504716148380624\n",
      "Delta 3.7622466524797105\n",
      "Delta 1.0833277487457948\n",
      "Delta 0.3074590637557577\n",
      "Delta 0.08544015624592305\n",
      "Delta 0.02346257670508578\n",
      "Delta 0.0064099250041067535\n",
      "Delta 0.0017480906637672433\n",
      "Delta 0.00047668347372109565\n",
      "Delta 0.0001300780044601879\n",
      "Delta 3.5534293701289243e-05\n",
      "Delta 9.719072593838973e-06\n",
      "Policy evalulation converged at 14\n",
      "Delta 28.104753594587464\n",
      "Delta 3.513094243643323\n",
      "Delta 0.5786324542685435\n",
      "Delta 0.16895658731688457\n",
      "Delta 0.027302904416359297\n",
      "Delta 0.008269866322820008\n",
      "Delta 0.002354798561597704\n",
      "Delta 0.0006440603004165268\n",
      "Delta 0.00017192872482496568\n",
      "Delta 4.5178914518828606e-05\n",
      "Delta 1.174640617307432e-05\n",
      "Delta 3.0315848249529154e-06\n",
      "Policy evalulation converged at 12\n",
      "Delta 7.783481663636849e-07\n",
      "Policy evalulation converged at 1\n",
      "-----------------------------------------\n",
      "Optimal extracted policy: \n",
      "[0, 2, 0, 1, 0, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 0, 0, 0, 0, 0, 1, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 0, 0, 0, 0, 1, 1, 0, 1, 1, 2, 0, 0, 0, 0, 0, 1, 0, 1, 3, 0, 0, 0]\n",
      "-----------------------------------------\n",
      "\n",
      "Location of Pacman:  [0, 0]\n",
      "Location of Ghost:  [0, 0]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [0, 0]\n",
      "Location of Ghost:  [1, 0]\n",
      "Best action determined by optimal policy:  DOWN\n",
      "\n",
      "Location of Pacman:  [0, 0]\n",
      "Location of Ghost:  [2, 0]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [0, 0]\n",
      "Location of Ghost:  [0, 1]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [0, 0]\n",
      "Location of Ghost:  [1, 1]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [0, 0]\n",
      "Location of Ghost:  [2, 1]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [0, 0]\n",
      "Location of Ghost:  [0, 2]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [0, 0]\n",
      "Location of Ghost:  [1, 2]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [0, 0]\n",
      "Location of Ghost:  [2, 2]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [1, 0]\n",
      "Location of Ghost:  [0, 0]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [1, 0]\n",
      "Location of Ghost:  [1, 0]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [1, 0]\n",
      "Location of Ghost:  [2, 0]\n",
      "Best action determined by optimal policy:  DOWN\n",
      "\n",
      "Location of Pacman:  [1, 0]\n",
      "Location of Ghost:  [0, 1]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [1, 0]\n",
      "Location of Ghost:  [1, 1]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [1, 0]\n",
      "Location of Ghost:  [2, 1]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [1, 0]\n",
      "Location of Ghost:  [0, 2]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [1, 0]\n",
      "Location of Ghost:  [1, 2]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [1, 0]\n",
      "Location of Ghost:  [2, 2]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [2, 0]\n",
      "Location of Ghost:  [0, 0]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [2, 0]\n",
      "Location of Ghost:  [1, 0]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [2, 0]\n",
      "Location of Ghost:  [2, 0]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [2, 0]\n",
      "Location of Ghost:  [0, 1]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [2, 0]\n",
      "Location of Ghost:  [1, 1]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [2, 0]\n",
      "Location of Ghost:  [2, 1]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [2, 0]\n",
      "Location of Ghost:  [0, 2]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [2, 0]\n",
      "Location of Ghost:  [1, 2]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [2, 0]\n",
      "Location of Ghost:  [2, 2]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [0, 1]\n",
      "Location of Ghost:  [0, 0]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [0, 1]\n",
      "Location of Ghost:  [1, 0]\n",
      "Best action determined by optimal policy:  LEFT\n",
      "\n",
      "Location of Pacman:  [0, 1]\n",
      "Location of Ghost:  [2, 0]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [0, 1]\n",
      "Location of Ghost:  [0, 1]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [0, 1]\n",
      "Location of Ghost:  [1, 1]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [0, 1]\n",
      "Location of Ghost:  [2, 1]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [0, 1]\n",
      "Location of Ghost:  [0, 2]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [0, 1]\n",
      "Location of Ghost:  [1, 2]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [0, 1]\n",
      "Location of Ghost:  [2, 2]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [1, 1]\n",
      "Location of Ghost:  [0, 0]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [1, 1]\n",
      "Location of Ghost:  [1, 0]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [1, 1]\n",
      "Location of Ghost:  [2, 0]\n",
      "Best action determined by optimal policy:  DOWN\n",
      "\n",
      "Location of Pacman:  [1, 1]\n",
      "Location of Ghost:  [0, 1]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [1, 1]\n",
      "Location of Ghost:  [1, 1]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [1, 1]\n",
      "Location of Ghost:  [2, 1]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [1, 1]\n",
      "Location of Ghost:  [0, 2]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [1, 1]\n",
      "Location of Ghost:  [1, 2]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [1, 1]\n",
      "Location of Ghost:  [2, 2]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [2, 1]\n",
      "Location of Ghost:  [0, 0]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [2, 1]\n",
      "Location of Ghost:  [1, 0]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [2, 1]\n",
      "Location of Ghost:  [2, 0]\n",
      "Best action determined by optimal policy:  LEFT\n",
      "\n",
      "Location of Pacman:  [2, 1]\n",
      "Location of Ghost:  [0, 1]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [2, 1]\n",
      "Location of Ghost:  [1, 1]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [2, 1]\n",
      "Location of Ghost:  [2, 1]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [2, 1]\n",
      "Location of Ghost:  [0, 2]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [2, 1]\n",
      "Location of Ghost:  [1, 2]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [2, 1]\n",
      "Location of Ghost:  [2, 2]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [0, 2]\n",
      "Location of Ghost:  [0, 0]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [0, 2]\n",
      "Location of Ghost:  [1, 0]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [0, 2]\n",
      "Location of Ghost:  [2, 0]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [0, 2]\n",
      "Location of Ghost:  [0, 1]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [0, 2]\n",
      "Location of Ghost:  [1, 1]\n",
      "Best action determined by optimal policy:  DOWN\n",
      "\n",
      "Location of Pacman:  [0, 2]\n",
      "Location of Ghost:  [2, 1]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [0, 2]\n",
      "Location of Ghost:  [0, 2]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [0, 2]\n",
      "Location of Ghost:  [1, 2]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [0, 2]\n",
      "Location of Ghost:  [2, 2]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [1, 2]\n",
      "Location of Ghost:  [0, 0]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [1, 2]\n",
      "Location of Ghost:  [1, 0]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [1, 2]\n",
      "Location of Ghost:  [2, 0]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [1, 2]\n",
      "Location of Ghost:  [0, 1]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [1, 2]\n",
      "Location of Ghost:  [1, 1]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [1, 2]\n",
      "Location of Ghost:  [2, 1]\n",
      "Best action determined by optimal policy:  DOWN\n",
      "\n",
      "Location of Pacman:  [1, 2]\n",
      "Location of Ghost:  [0, 2]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [1, 2]\n",
      "Location of Ghost:  [1, 2]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [1, 2]\n",
      "Location of Ghost:  [2, 2]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [2, 2]\n",
      "Location of Ghost:  [0, 0]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [2, 2]\n",
      "Location of Ghost:  [1, 0]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [2, 2]\n",
      "Location of Ghost:  [2, 0]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [2, 2]\n",
      "Location of Ghost:  [0, 1]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [2, 2]\n",
      "Location of Ghost:  [1, 1]\n",
      "Best action determined by optimal policy:  RIGHT\n",
      "\n",
      "Location of Pacman:  [2, 2]\n",
      "Location of Ghost:  [2, 1]\n",
      "Best action determined by optimal policy:  LEFT\n",
      "\n",
      "Location of Pacman:  [2, 2]\n",
      "Location of Ghost:  [0, 2]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [2, 2]\n",
      "Location of Ghost:  [1, 2]\n",
      "Best action determined by optimal policy:  UP\n",
      "\n",
      "Location of Pacman:  [2, 2]\n",
      "Location of Ghost:  [2, 2]\n",
      "Best action determined by optimal policy:  UP\n",
      "-----------------------------------------\n",
      "Value Function: \n",
      "[0, 68.47795820871247, 65.43139606003761, 500.0, 174.07354723961527, 324.11041972926375, 500.0, 500.0, 437.5, 1000.0, 0, 92.8833578724859, 1000.0, 1000.0, 500.0, 1000.0, 1000.0, 1000.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 212.38880246615798, 68.47795820904145, 92.8833578724859, 0, 174.07354724224703, 125.13942037861457, 250.0, 201.4466934049519, 212.38880246615798, 437.5, 324.11041973406077, 47.423011779227444, 500.0, 0, 324.11041973406077, 500.0, 500.0, 437.5, 1000.0, 500.0, 92.88335787832204, 1000.0, 1000.0, 0, 1000.0, 1000.0, 1000.0, 93.9204648253604, 75.56064098096344, 47.423011779227444, 104.73863038929139, 45.07481784162006, 75.56064099493818, 0, 104.73863038916961, 93.9204648394853, 212.3888024667576, 125.13942037642434, 92.88335787832204, 201.4466934054918, 174.07354724496224, 68.47795822657014, 250.0, 0, 212.3888024667576, 437.5, 324.11041973479024, 65.4313962534033, 500.0, 174.07354724505342, 68.4779582270765, 500.0, 500.0, 0]\n",
      "Steps 'til convergence:  6\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "### Policy iteration result ###\n",
    "policy2, V2, steps2 = policy_iteration(gamma=0.5)\n",
    "env = PacmanEnv()\n",
    "action_list2 = []\n",
    "\n",
    "print('-----------------------------------------')\n",
    "print('Optimal extracted policy: ')\n",
    "print(policy2)\n",
    "\n",
    "print('-----------------------------------------')\n",
    "for s in range(env.num_states):\n",
    "    print()\n",
    "    print('Location of Pacman: ', grid_to_xy(env.states[s][0]))\n",
    "    print('Location of Ghost: ', grid_to_xy(env.states[s][1]))\n",
    "    print('Best action determined by optimal policy: ', convert_to_action( policy2[s] ) )\n",
    "    action_list2.append(convert_to_action( policy2[s] ))\n",
    "\n",
    "print('-----------------------------------------')    \n",
    "print('Value Function: ')\n",
    "print(V2)\n",
    "\n",
    "print('Steps \\'til convergence: ', steps2)\n",
    "print('-----------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Different at state  0 ---------\n",
      "Value iteration optimal policy:  RIGHT\n",
      "Policy iteration optimal policy:  UP\n",
      "---------Different at state  2 ---------\n",
      "Value iteration optimal policy:  RIGHT\n",
      "Policy iteration optimal policy:  UP\n",
      "---------Different at state  4 ---------\n",
      "Value iteration optimal policy:  RIGHT\n",
      "Policy iteration optimal policy:  UP\n",
      "---------Different at state  10 ---------\n",
      "Value iteration optimal policy:  RIGHT\n",
      "Policy iteration optimal policy:  UP\n"
     ]
    }
   ],
   "source": [
    "diff_opt_states = []\n",
    "\n",
    "for s in range(env.num_states):\n",
    "    if (action_list[s] != action_list2[s]):\n",
    "        diff_opt_states.append(s)\n",
    "\n",
    "for i in range(len(diff_opt_states)):\n",
    "    if (action_list[i] != action_list2[i]):\n",
    "        print('---------Different at state ', i, '---------')\n",
    "        print('Value iteration optimal policy: ',action_list[i])\n",
    "        print('Policy iteration optimal policy: ',action_list2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
